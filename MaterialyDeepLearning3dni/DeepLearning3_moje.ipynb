{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import nltk\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 75\n",
    "\n",
    "with open(\"Dane/kod.txt\") as f:\n",
    "    sentences = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [s for s in sentences if s.strip() != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['class RNNNumpy:\\n',\n",
       " '    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\\n',\n",
       " '        # Assign instance variables\\n',\n",
       " '        self.word_dim = word_dim\\n',\n",
       " '        self.hidden_dim = hidden_dim\\n',\n",
       " '        self.bptt_truncate = bptt_truncate\\n',\n",
       " '        # Randomly initialize the network parameters\\n',\n",
       " '        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\\n',\n",
       " '        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\\n',\n",
       " '        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [list(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['c',\n",
       "  'l',\n",
       "  'a',\n",
       "  's',\n",
       "  's',\n",
       "  ' ',\n",
       "  'R',\n",
       "  'N',\n",
       "  'N',\n",
       "  'N',\n",
       "  'u',\n",
       "  'm',\n",
       "  'p',\n",
       "  'y',\n",
       "  ':',\n",
       "  '\\n'],\n",
       " [' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  'd',\n",
       "  'e',\n",
       "  'f',\n",
       "  ' ',\n",
       "  '_',\n",
       "  '_',\n",
       "  'i',\n",
       "  'n',\n",
       "  'i',\n",
       "  't',\n",
       "  '_',\n",
       "  '_',\n",
       "  '(',\n",
       "  's',\n",
       "  'e',\n",
       "  'l',\n",
       "  'f',\n",
       "  ',',\n",
       "  ' ',\n",
       "  'w',\n",
       "  'o',\n",
       "  'r',\n",
       "  'd',\n",
       "  '_',\n",
       "  'd',\n",
       "  'i',\n",
       "  'm',\n",
       "  ',',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'i',\n",
       "  'd',\n",
       "  'd',\n",
       "  'e',\n",
       "  'n',\n",
       "  '_',\n",
       "  'd',\n",
       "  'i',\n",
       "  'm',\n",
       "  '=',\n",
       "  '1',\n",
       "  '0',\n",
       "  '0',\n",
       "  ',',\n",
       "  ' ',\n",
       "  'b',\n",
       "  'p',\n",
       "  't',\n",
       "  't',\n",
       "  '_',\n",
       "  't',\n",
       "  'r',\n",
       "  'u',\n",
       "  'n',\n",
       "  'c',\n",
       "  'a',\n",
       "  't',\n",
       "  'e',\n",
       "  '=',\n",
       "  '4',\n",
       "  ')',\n",
       "  ':',\n",
       "  '\\n'],\n",
       " [' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  ' ',\n",
       "  '#',\n",
       "  ' ',\n",
       "  'A',\n",
       "  's',\n",
       "  's',\n",
       "  'i',\n",
       "  'g',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'i',\n",
       "  'n',\n",
       "  's',\n",
       "  't',\n",
       "  'a',\n",
       "  'n',\n",
       "  'c',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'v',\n",
       "  'a',\n",
       "  'r',\n",
       "  'i',\n",
       "  'a',\n",
       "  'b',\n",
       "  'l',\n",
       "  'e',\n",
       "  's',\n",
       "  '\\n']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vocabulary size 75.\n",
      "\n",
      "Example sentence: 'class RNNNumpy:\n",
      "'\n",
      "\n",
      "Example sentence after Pre-processing: '['c', 'l', 'a', 's', 's', ' ', 'R', 'N', 'N', 'N', 'u', 'm', 'p', 'y', ':', '\\n']'\n",
      "[50, 59, 48, 66, 66, 1, 37, 34, 34, 34, 68, 60, 63, 72, 19]\n",
      "[59, 48, 66, 66, 1, 37, 34, 34, 34, 68, 60, 63, 72, 19, 0]\n"
     ]
    }
   ],
   "source": [
    "vocab = np.unique([item for sublist in tokenized_sentences for item in sublist]) \n",
    "index_to_word = [x[0] for x in vocab]\n",
    "\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    " \n",
    "print( \"Using vocabulary size %d.\" % len(vocab))\n",
    "\n",
    "# Create the training data\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n",
    "\n",
    "\n",
    "print( \"\\nExample sentence: '%s'\" % sentences[0])\n",
    "print( \"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0])\n",
    "print( X_train[0])\n",
    "print( y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNNumpy:\n",
    "     \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(self, x):\n",
    "    # The total number of time steps\n",
    "    T = len(x)\n",
    "    # During forward propagation we save all hidden states in s because need them later.\n",
    "    # We add one additional element for the initial hidden, which we set to 0\n",
    "    h = np.zeros((T + 1, self.hidden_dim))\n",
    "    h[-1] = np.zeros(self.hidden_dim)\n",
    "    # The outputs at each time step. Again, we save them for later.\n",
    "    o = np.zeros((T, self.word_dim))\n",
    "    # For each time step...\n",
    "    for t in np.arange(T):\n",
    "        # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "         \n",
    "        x_t =  np.eye(len(vocab))[x[t]] # Kodowanie one-hot. \n",
    "                                        # Formalnie rzecz biorac, x_t jest gotowym wektorem wprowadzanym do sieci,\n",
    "                                        # ale dla wygody tu przetwarzamy indeks na wektor\n",
    "        \n",
    "        h[t] = np.tanh(np.dot(self.U, x_t) + np.dot(self.W, h[t-1])) # obliczanie stanu ukrytego\n",
    "        o[t] = softmax(np.dot(self.V, h[t])) # predykcja dla x_t\n",
    "                                             # uwaga: w zalenosci od problemu bedzie nas interesowac \n",
    "                                             #        kazda predykcja lub tylko ostatnia\n",
    "        \n",
    "    return [o, h]\n",
    " \n",
    "RNNNumpy.forward_propagation = forward_propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, x):\n",
    "    # Perform forward propagation and return index of the highest score\n",
    "    o, s = self.forward_propagation(x)\n",
    "    return np.argmax(o, axis=1)\n",
    " \n",
    "RNNNumpy.predict = predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[51,\n",
       " 52,\n",
       " 53,\n",
       " 1,\n",
       " 53,\n",
       " 62,\n",
       " 65,\n",
       " 70,\n",
       " 48,\n",
       " 65,\n",
       " 51,\n",
       " 47,\n",
       " 63,\n",
       " 65,\n",
       " 62,\n",
       " 63,\n",
       " 48,\n",
       " 54,\n",
       " 48,\n",
       " 67,\n",
       " 56,\n",
       " 62,\n",
       " 61,\n",
       " 6,\n",
       " 66,\n",
       " 52,\n",
       " 59,\n",
       " 53,\n",
       " 10,\n",
       " 1,\n",
       " 71,\n",
       " 7,\n",
       " 19]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 75)\n",
      "[[0.0137885  0.01259464 0.01312937 ... 0.0121929  0.01291467 0.01447739]\n",
      " [0.01307076 0.01411536 0.01411408 ... 0.01378971 0.01418804 0.01405048]\n",
      " [0.01281134 0.01272381 0.01326489 ... 0.01275765 0.01318253 0.01373512]\n",
      " ...\n",
      " [0.01314944 0.01428113 0.01362903 ... 0.01324931 0.0143878  0.01310582]\n",
      " [0.013139   0.01351207 0.0123365  ... 0.01295007 0.01473019 0.0127632 ]\n",
      " [0.01313058 0.01459512 0.01356838 ... 0.01243141 0.01246906 0.01237279]]\n"
     ]
    }
   ],
   "source": [
    "# Test:\n",
    "\n",
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "o, s = model.forward_propagation(X_train[10])\n",
    "print(o.shape)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33,)\n",
      "[58 68  5 56 45 67 11 42 69 11 26 48 58 11 53  1 68 15 69 52 20 24 46 27\n",
      " 34 21 24  5 57 56 69 73 49]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_train[10])\n",
    "print(predictions.shape)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_loss(self, x, y):\n",
    "    L = 0\n",
    "    # For each sentence...\n",
    "    for i in np.arange(len(y)):\n",
    "        o, s = self.forward_propagation(x[i])\n",
    "        # We only care about our prediction of the \"correct\" words\n",
    "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "        # Add to the loss based on how off we were\n",
    "        L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "    return L\n",
    " \n",
    "def calculate_loss(self, x, y):\n",
    "    # Divide the total loss by the number of training examples\n",
    "    N = np.sum((len(y_i) for y_i in y))\n",
    "    return self.calculate_total_loss(x,y)/N\n",
    "\n",
    "RNNNumpy.calculate_total_loss = calculate_total_loss\n",
    "RNNNumpy.calculate_loss = calculate_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for random predictions: 4.317488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olszewskip/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.from_iter(generator)) or the python sum builtin instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual loss: 4.324395\n"
     ]
    }
   ],
   "source": [
    "# Limit to 1000 examples to save time\n",
    "print(\"Loss for random predictions: %f\" % np.log(vocabulary_size))\n",
    "print(\"Actual loss: %f\" % model.calculate_loss(X_train[:1000], y_train[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    # Perform forward propagation\n",
    "    o, s = self.forward_propagation(x)\n",
    "    \n",
    "    # We accumulate the gradients in these variables\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    \n",
    "    # For each output backwards...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "        # Initial delta calculation\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "            dLdU[:,x[bptt_step]] += delta_t\n",
    "            # Update delta for next step\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "    return [dLdU, dLdV, dLdW]\n",
    " \n",
    "RNNNumpy.bptt = bptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.9 ms ± 1.46 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "\n",
    "# Performs one step of SGD.\n",
    "def numpy_sdg_step(self, x, y, learning_rate):\n",
    "    # Calculate the gradients\n",
    "    dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "    # Change parameters according to gradients and learning rate\n",
    "    self.U -= learning_rate * dLdU\n",
    "    self.V -= learning_rate * dLdV\n",
    "    self.W -= learning_rate * dLdW\n",
    " \n",
    "RNNNumpy.sgd_step = numpy_sdg_step\n",
    "# Outer SGD Loop\n",
    "# - model: The RNN model instance\n",
    "# - X_train: The training data set\n",
    "# - y_train: The training data labels\n",
    "# - learning_rate: Initial learning rate for SGD\n",
    "# - nepoch: Number of times to iterate through the complete dataset\n",
    "# - evaluate_loss_after: Evaluate the loss after this many epochs\n",
    "def train_with_sgd(self, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # Optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            \n",
    "            # Adjust the learning rate if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5 \n",
    "                print(\"Setting learning rate to %f\" % learning_rate)\n",
    "            sys.stdout.flush()\n",
    "        # For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            # One SGD step\n",
    "            self.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1\n",
    "    return(losses)\n",
    "\n",
    "RNNNumpy.train_with_sgd = train_with_sgd\n",
    "            \n",
    "np.random.seed(10)\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "%timeit model.sgd_step(X_train[10], y_train[10], 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/olszewskip/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.from_iter(generator)) or the python sum builtin instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting learning rate to 0.002500\n",
      "Setting learning rate to 0.001250\n",
      "Setting learning rate to 0.000625\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "# Train on a small subset of the data to see what happens\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "losses = model.train_with_sgd(X_train, y_train, nepoch=100, evaluate_loss_after=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(s,n=3):\n",
    "    \"\"\"\n",
    "    \n",
    "    Funkcja przewiduje n nastepnych liter dla podanegou tekstu (napisu) s.\n",
    "    \n",
    "    \"\"\"\n",
    "    s = list(s)\n",
    "    \n",
    "    X_new = np.asarray([[word_to_index[w] for w in sent] for sent in s])[:,0]\n",
    "    \n",
    "    pred = np.zeros(n,dtype=\"int\")\n",
    "    for i in range(n):\n",
    "        pred[i] = model.predict(np.concatenate([X_new, pred[:i]]))[-1]\n",
    "    print(\"original: \", ''.join([x[0] for x in s]) )\n",
    "    print(\"prediction: \", ''.join([x[0] for x in s])+''.join([[index_to_word[w] for w in sent] for sent in [pred]][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testowanie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original:  for\n",
      "prediction:  formarn_pros = err\n"
     ]
    }
   ],
   "source": [
    "generate_text(\"for\", 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddingi\n",
    "\n",
    "Przeanalizujmy co się dzieje w RNN, gdy podajemy słowa w reprezentacji one hot.\n",
    "\n",
    "## $$ h_t = f( W^h * h_{t-1} + W^x * x_t + b)$$\n",
    "\n",
    "Zatem jeśli x to \"one-hot\" z jedynką na pozycji $i$ to:\n",
    "\n",
    "## $$ W^x * x_t = W^x[:,i],  $$\n",
    "\n",
    "Czyli wkład informacji embeddinga sprowadza się do wzięcia odpowieniej kolumny macierzy wag.\n",
    "\n",
    "Czyli i-ta kolumna macierzy wag jest w pewnym sensie reprezentacją słowa i.\n",
    "\n",
    "Zatem pójdźmy krok dalej: stwórzmy sobie dodatkową warstwę w sieci, zawierającą reprezentacje słów, które będą przekazywane do wyliczenia stanu ukrytego.\n",
    "\n",
    "Wówczas sieć z warstwą embeddingów ma postać:\n",
    "\n",
    "<br>\n",
    "\n",
    "$x_t$ - id słowa wejściowego w momencie $t$.\n",
    "\n",
    "$EMB$ - macierz embeddingów\n",
    "\n",
    "<br>\n",
    "\n",
    "$$emb_t = EMB[x_t]$$\n",
    "$$ h_t = f( W^h * h_{t-1} + W^x * emb_t + b)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Ta warstwa nazywa się EMBEDDING'ami (embedding layer).\n",
    "\n",
    "\n",
    "<img src=\"Grafika/embeddings.jpg\" width=\"700\">\n",
    "Źródło: https://www.slideshare.net/Geeks_Lab/aibigdata-lab-2016-62764857\n",
    "\n",
    "\n",
    "\n",
    "### Zauważmy, że embeddingi są parametrami sieci, ale jednocześnie reprezentacją słów. Oznacza to, że trenując sieć, uczymy embeddingi, czyli uczymy się reprezentacji słów.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case study: IMBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, SimpleRNN, LSTM, Bidirectional\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 8s 0us/step\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "[list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32])\n",
      " list([1, 194, 1153, 194, 2, 78, 228, 5, 6, 1463, 4369, 2, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 2, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 2, 2, 349, 2637, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 2, 5, 2, 656, 245, 2350, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95])\n",
      " list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 2, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113])]\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print(x_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ujednolicenie długości ciągów - padding:\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train,maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    1,   14,   22,   16,   43,\n",
       "        530,  973, 1622, 1385,   65,  458, 4468,   66, 3941,    4,  173,\n",
       "         36,  256,    5,   25,  100,   43,  838,  112,   50,  670,    2,\n",
       "          9,   35,  480,  284,    5,  150,    4,  172,  112,  167,    2,\n",
       "        336,  385,   39,    4,  172, 4536, 1111,   17,  546,   38,   13,\n",
       "        447,    4,  192,   50,   16,    6,  147, 2025,   19,   14,   22,\n",
       "          4, 1920, 4613,  469,    4,   22,   71,   87,   12,   16,   43,\n",
       "        530,   38,   76,   15,   13, 1247,    4,   22,   17,  515,   17,\n",
       "         12,   16,  626,   18,    2,    5,   62,  386,   12,    8,  316,\n",
       "          8,  106,    5,    4, 2223,    2,   16,  480,   66, 3785,   33,\n",
       "          4,  130,   12,   16,   38,  619,    5,   25,  124,   51,   36,\n",
       "        135,   48,   25, 1415,   33,    6,   22,   12,  215,   28,   77,\n",
       "         52,    5,   14,  407,   16,   82,    2,    8,    4,  107,  117,\n",
       "          2,   15,  256,    4,    2,    7, 3766,    5,  723,   36,   71,\n",
       "         43,  530,  476,   26,  400,  317,   46,    7,    4,    2, 1029,\n",
       "         13,  104,   88,    4,  381,   15,  297,   98,   32, 2071,   56,\n",
       "         26,  141,    6,  194,    2,   18,    4,  226,   22,   21,  134,\n",
       "        476,   26,  480,    5,  144,   30,    2,   18,   51,   36,   28,\n",
       "        224,   92,   25,  104,    4,  226,   65,   16,   38, 1334,   88,\n",
       "         12,   16,  283,    5,   16, 4472,  113,  103,   32,   15,   16,\n",
       "          2,   19,  178,   32], dtype=int32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 5000\n",
    "n_test = 2000\n",
    "x_train = x_train[:n_train]\n",
    "y_train = y_train[:n_train]\n",
    "x_test = x_test[:n_test]\n",
    "y_test = y_test[:n_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zwykła sieć rekurencyjna ( z embeddingami)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 400, 50)           250000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_4 (SimpleRNN)     (None, 100)               15100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 265,201\n",
      "Trainable params: 265,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add( Embedding( max_features, embedding_dims, input_length=maxlen ) )\n",
    "model.add(SimpleRNN(100, dropout=0.5))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "4000/4000 [==============================] - 19s 5ms/step - loss: 0.6959 - acc: 0.5085 - val_loss: 0.6820 - val_acc: 0.5850\n",
      "Epoch 2/100\n",
      "4000/4000 [==============================] - 21s 5ms/step - loss: 0.6571 - acc: 0.6398 - val_loss: 0.6654 - val_acc: 0.6200\n",
      "Epoch 3/100\n",
      "4000/4000 [==============================] - 22s 5ms/step - loss: 0.6446 - acc: 0.6392 - val_loss: 0.6624 - val_acc: 0.6430\n",
      "Epoch 4/100\n",
      "4000/4000 [==============================] - 23s 6ms/step - loss: 0.6470 - acc: 0.6577 - val_loss: 0.6692 - val_acc: 0.5540\n",
      "Epoch 5/100\n",
      "4000/4000 [==============================] - 22s 6ms/step - loss: 0.6146 - acc: 0.7080 - val_loss: 0.6415 - val_acc: 0.6260\n",
      "Epoch 6/100\n",
      "4000/4000 [==============================] - 22s 5ms/step - loss: 0.5694 - acc: 0.7288 - val_loss: 0.6425 - val_acc: 0.6260\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f696f1ae6d8>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(patience=3, monitor=\"val_acc\")\n",
    "model.fit(x_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 2s 795us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6563675355911255, 0.608]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 400, 50)           250000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_5 (SimpleRNN)     (None, 100)               15100     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 270,201\n",
      "Trainable params: 270,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "4000/4000 [==============================] - 19s 5ms/step - loss: 0.7037 - acc: 0.4978 - val_loss: 0.6915 - val_acc: 0.5570\n",
      "Epoch 2/100\n",
      "4000/4000 [==============================] - 21s 5ms/step - loss: 0.6935 - acc: 0.5205 - val_loss: 0.6931 - val_acc: 0.5040\n",
      "Epoch 3/100\n",
      "4000/4000 [==============================] - 22s 6ms/step - loss: 0.6903 - acc: 0.5310 - val_loss: 0.6866 - val_acc: 0.5710\n",
      "Epoch 4/100\n",
      "4000/4000 [==============================] - 22s 6ms/step - loss: 0.6802 - acc: 0.5793 - val_loss: 0.6871 - val_acc: 0.5350\n",
      "Epoch 5/100\n",
      "4000/4000 [==============================] - 22s 6ms/step - loss: 0.6442 - acc: 0.6422 - val_loss: 0.6450 - val_acc: 0.5910\n",
      "Epoch 6/100\n",
      "4000/4000 [==============================] - 22s 6ms/step - loss: 0.5581 - acc: 0.7218 - val_loss: 0.6933 - val_acc: 0.5990\n",
      "Epoch 7/100\n",
      "4000/4000 [==============================] - 22s 5ms/step - loss: 0.5152 - acc: 0.7412 - val_loss: 0.6279 - val_acc: 0.6680\n",
      "Epoch 8/100\n",
      "4000/4000 [==============================] - 22s 6ms/step - loss: 0.4372 - acc: 0.7993 - val_loss: 0.6439 - val_acc: 0.6800\n",
      "Epoch 9/100\n",
      "4000/4000 [==============================] - 23s 6ms/step - loss: 0.3753 - acc: 0.8380 - val_loss: 0.6606 - val_acc: 0.6900\n",
      "Epoch 10/100\n",
      "4000/4000 [==============================] - 22s 6ms/step - loss: 0.3482 - acc: 0.8538 - val_loss: 0.6850 - val_acc: 0.6920\n",
      "Epoch 11/100\n",
      "4000/4000 [==============================] - 22s 6ms/step - loss: 0.2836 - acc: 0.8832 - val_loss: 0.7863 - val_acc: 0.6840\n",
      "Epoch 12/100\n",
      "4000/4000 [==============================] - 22s 6ms/step - loss: 0.2678 - acc: 0.8915 - val_loss: 0.7596 - val_acc: 0.6910\n",
      "Epoch 13/100\n",
      "4000/4000 [==============================] - 22s 6ms/step - loss: 0.2472 - acc: 0.9073 - val_loss: 0.8693 - val_acc: 0.6460\n",
      "2000/2000 [==============================] - 2s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.906218529701233, 0.631]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add( Embedding( max_features, embedding_dims, input_length=maxlen ) )\n",
    "model.add(SimpleRNN(100, dropout=0.5))\n",
    "model.add(Dense(50,activation='sigmoid'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(patience=3, monitor=\"val_acc\")\n",
    "model.fit(x_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 400, 50)           250000    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200)               30200     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                10050     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 290,301\n",
      "Trainable params: 290,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "4000/4000 [==============================] - 39s 10ms/step - loss: 0.6966 - acc: 0.5072 - val_loss: 0.6909 - val_acc: 0.5270\n",
      "Epoch 2/100\n",
      "4000/4000 [==============================] - 42s 10ms/step - loss: 0.6930 - acc: 0.5160 - val_loss: 0.7529 - val_acc: 0.4900\n",
      "Epoch 3/100\n",
      "4000/4000 [==============================] - 42s 10ms/step - loss: 0.6974 - acc: 0.5185 - val_loss: 0.6940 - val_acc: 0.5010\n",
      "Epoch 4/100\n",
      "4000/4000 [==============================] - 42s 10ms/step - loss: 0.6925 - acc: 0.5307 - val_loss: 0.6907 - val_acc: 0.5200\n",
      "2000/2000 [==============================] - 4s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.686422926902771, 0.5455]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add( Embedding( max_features, embedding_dims, input_length=maxlen ) )\n",
    "model.add(Bidirectional(SimpleRNN(100, dropout=0.5)))\n",
    "model.add(Dense(50,activation='sigmoid'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(patience=3, monitor=\"val_acc\")\n",
    "model.fit(x_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 400, 50)           250000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_11 (SimpleRNN)    (None, 400, 10)           610       \n",
      "_________________________________________________________________\n",
      "simple_rnn_12 (SimpleRNN)    (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 251,251\n",
      "Trainable params: 251,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "4000/4000 [==============================] - 26s 7ms/step - loss: 0.6181 - acc: 0.6440 - val_loss: 0.5312 - val_acc: 0.7360\n",
      "Epoch 2/100\n",
      "4000/4000 [==============================] - 27s 7ms/step - loss: 0.3368 - acc: 0.8632 - val_loss: 0.5036 - val_acc: 0.7660\n",
      "Epoch 3/100\n",
      "4000/4000 [==============================] - 26s 7ms/step - loss: 0.1376 - acc: 0.9565 - val_loss: 0.6009 - val_acc: 0.7680\n",
      "Epoch 4/100\n",
      "4000/4000 [==============================] - 27s 7ms/step - loss: 0.0561 - acc: 0.9828 - val_loss: 0.7135 - val_acc: 0.7590\n",
      "Epoch 5/100\n",
      "4000/4000 [==============================] - 27s 7ms/step - loss: 0.0688 - acc: 0.9795 - val_loss: 0.7413 - val_acc: 0.7750\n",
      "Epoch 6/100\n",
      "4000/4000 [==============================] - 26s 7ms/step - loss: 0.0212 - acc: 0.9963 - val_loss: 0.8409 - val_acc: 0.7620\n",
      "Epoch 7/100\n",
      "4000/4000 [==============================] - 26s 6ms/step - loss: 0.0505 - acc: 0.9843 - val_loss: 0.7966 - val_acc: 0.7400\n",
      "Epoch 8/100\n",
      "4000/4000 [==============================] - 27s 7ms/step - loss: 0.0285 - acc: 0.9915 - val_loss: 0.8732 - val_acc: 0.7370\n",
      "2000/2000 [==============================] - 2s 974us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8264236125946045, 0.7535]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add( Embedding( max_features, embedding_dims, input_length=maxlen ) )\n",
    "model.add(SimpleRNN(10, return_sequences=True))\n",
    "model.add(SimpleRNN(20))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(patience=3, monitor=\"val_acc\")\n",
    "model.fit(x_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 400, 50)           250000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 20)                5680      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 255,701\n",
      "Trainable params: 255,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "4000/4000 [==============================] - 60s 15ms/step - loss: 0.6080 - acc: 0.6735 - val_loss: 0.4750 - val_acc: 0.7850\n",
      "Epoch 2/100\n",
      "4000/4000 [==============================] - 63s 16ms/step - loss: 0.3466 - acc: 0.8645 - val_loss: 0.4333 - val_acc: 0.8150\n",
      "Epoch 3/100\n",
      "4000/4000 [==============================] - 63s 16ms/step - loss: 0.2355 - acc: 0.9147 - val_loss: 0.4410 - val_acc: 0.8130\n",
      "Epoch 4/100\n",
      "4000/4000 [==============================] - 63s 16ms/step - loss: 0.1658 - acc: 0.9420 - val_loss: 0.4694 - val_acc: 0.8100\n",
      "Epoch 5/100\n",
      " 832/4000 [=====>........................] - ETA: 49s - loss: 0.1002 - acc: 0.9724"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-5f2ac77316e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val_acc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add( Embedding( max_features, embedding_dims, input_length=maxlen ) )\n",
    "model.add(LSTM(20))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(patience=3, monitor=\"val_acc\")\n",
    "model.fit(x_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model word2vec \n",
    "\n",
    "<img src=\"https://deeplearning4j.org/img/word2vec_diagrams.png\" width=\"700\">\n",
    "Źródło: https://deeplearning4j.org/img/word2vec_diagrams.png\n",
    "\n",
    "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/06062705/Word-Vectors.png\" width=\"700\">\n",
    "Źródło: https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/06062705/Word-Vectors.png\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case study: Analiza sentymentu\n",
    "\n",
    "Znaleźć najlepszą sieć. Przykładowe warianty:\n",
    "\n",
    "1. Simple RNN\n",
    "2. LSTM - porównaj na zbiorze testowym jakość działania modelu wziętego z najlepszej iteracji oraz modelu po zatrzymaniu uczenia\n",
    "3. LSTM + warstwa/y dense na końcu\n",
    "4. BiLSTM\n",
    "5. dwuwarstwowy LSTM\n",
    "6. dwuwartwowy BiLSTM + dense\n",
    "7. CNN + LSTM - przepuścić dane przez warstwę konwolucyjną - conv1d (+ max pooling), a następnie przejechać LSTM'em po tym wyszło."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 300)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "file_with_filtered_embeddings = \"Dane/data_poleval/embeddings.txt\"\n",
    "\n",
    "words2ids = {}\n",
    "embeddings = []\n",
    "\n",
    "embeddings.append(np.zeros(300)) # rezerwujemy embeddingi na paddin i nieznane slowa\n",
    "embeddings.append(np.zeros(300))\n",
    "\n",
    "i = 0\n",
    "with open(file_with_filtered_embeddings,\"r\") as f:\n",
    "    for line in f:\n",
    "        toks = line.split(\" \")\n",
    "        word = toks[0]\n",
    "        embeddings.append(np.array([float(x) for x in toks[1:]]))\n",
    "        words2ids[word] = i+2 # +3 - przesuniecie po to zeby specjalne embeddingi byly na pozycji 0 i 1\n",
    "        i = i + 1\n",
    "\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mają': 2,\n",
       " 'cytrusów': 3,\n",
       " 'ponadto': 4,\n",
       " 'pięknej': 5,\n",
       " 'pudrowego': 6,\n",
       " 'produkt': 7,\n",
       " 'brodzików': 8,\n",
       " 'U': 9,\n",
       " 'podziwu': 10,\n",
       " 'słowem': 11,\n",
       " 'diabłem': 12,\n",
       " 'malarze': 13,\n",
       " 'mój': 14,\n",
       " 'przestać': 15,\n",
       " 'whiskey': 16,\n",
       " 'żadnej': 17,\n",
       " 'Swój': 18,\n",
       " 'pieprzową': 19,\n",
       " 'figury': 20,\n",
       " 'kupować': 21,\n",
       " 'pozostałych': 22,\n",
       " 'komplety': 23,\n",
       " 'zamachu': 24,\n",
       " 'dalekosiężne': 25,\n",
       " 'leżą': 26,\n",
       " 'aprobatą': 27,\n",
       " 'bujnym': 28,\n",
       " 'podobieństwa': 29,\n",
       " 'sukienki': 30,\n",
       " 'uciemiężone': 31,\n",
       " 'koszulę': 32,\n",
       " 'koszuli': 33,\n",
       " 'koszule': 34,\n",
       " 'dłużej': 35,\n",
       " 'wypuści': 36,\n",
       " 'tworząc': 37,\n",
       " 'rekompensuje': 38,\n",
       " 'przecudowny': 39,\n",
       " 'wyróżniających': 40,\n",
       " 'należytemu': 41,\n",
       " 'konkretnych': 42,\n",
       " 'święto': 43,\n",
       " 'patologiem': 44,\n",
       " 'przeciętnej': 45,\n",
       " 'Donatellę': 46,\n",
       " 'powstańcza': 47,\n",
       " 'testu': 48,\n",
       " 'mieszkania': 49,\n",
       " 'tę': 50,\n",
       " 'adekwatna': 51,\n",
       " 'oczekiwaniami': 52,\n",
       " 'natomiast': 53,\n",
       " 'polecenia': 54,\n",
       " 'nasz': 55,\n",
       " 'Można': 56,\n",
       " 'Uwielbiam': 57,\n",
       " 'sprawców': 58,\n",
       " 'Używała': 59,\n",
       " 'wyjątkowy': 60,\n",
       " 'kolorze': 61,\n",
       " 'lekarzy': 62,\n",
       " 'lekarze': 63,\n",
       " 'ogólnie': 64,\n",
       " 'wody': 65,\n",
       " 'wywołuje': 66,\n",
       " 'wywołują': 67,\n",
       " 'zielony': 68,\n",
       " 'zielono': 69,\n",
       " 'zielone': 70,\n",
       " 'podróbki': 71,\n",
       " 'natarczywą': 72,\n",
       " 'Niska': 73,\n",
       " 'wychodzą': 74,\n",
       " 'wtedy': 75,\n",
       " 'wyróżnia': 76,\n",
       " 'patrzyły': 77,\n",
       " 'opcja': 78,\n",
       " 'gustownie': 79,\n",
       " 'boski': 80,\n",
       " 'Ducha': 81,\n",
       " 'orientalnych': 82,\n",
       " 'wartość': 83,\n",
       " 'najgorszy': 84,\n",
       " 'cechy': 85,\n",
       " 'zmienili': 86,\n",
       " 'Niepowtarzalny': 87,\n",
       " 'własny': 88,\n",
       " 'amerykańskiej': 89,\n",
       " 'oblicza': 90,\n",
       " 'oblicze': 91,\n",
       " 'stylu': 92,\n",
       " 'bawełniane': 93,\n",
       " 'problemy': 94,\n",
       " 'problemu': 95,\n",
       " 'Przez': 96,\n",
       " 'Przed': 97,\n",
       " 'zainteresuje': 98,\n",
       " 'biorą': 99,\n",
       " 'przeznaczona': 100,\n",
       " 'czuła': 101,\n",
       " 'przestępców': 102,\n",
       " 'aktywność': 103,\n",
       " 'rewolucyjnym': 104,\n",
       " 'towarzyszyły': 105,\n",
       " 'ubraniem': 106,\n",
       " 'wypróbować': 107,\n",
       " 'lubiła': 108,\n",
       " 'kiedy': 109,\n",
       " 'uważnie': 110,\n",
       " 'Zróżnicowana': 111,\n",
       " 'upływie': 112,\n",
       " 'skąpo': 113,\n",
       " 'Miło': 114,\n",
       " 'pomimo': 115,\n",
       " 'Nie': 116,\n",
       " 'powiadomić': 117,\n",
       " 'flakonie': 118,\n",
       " 'flakonik': 119,\n",
       " 'najczulszym': 120,\n",
       " 'lustrował': 121,\n",
       " 'kosztują': 122,\n",
       " 'Gilada': 123,\n",
       " 'trawą': 124,\n",
       " 'choinkę': 125,\n",
       " 'namiętny': 126,\n",
       " 'kojarzyła': 127,\n",
       " 'swojego': 128,\n",
       " 'dobrze': 129,\n",
       " 'działa': 130,\n",
       " 'zauważył': 131,\n",
       " 'powagi': 132,\n",
       " 'Minusem': 133,\n",
       " 'cytoplazmatyczne': 134,\n",
       " 'podobnym': 135,\n",
       " 'straty': 136,\n",
       " 'jedynie': 137,\n",
       " 'pozbawioną': 138,\n",
       " 'pozbawiona': 139,\n",
       " 'przestępstwa': 140,\n",
       " 'naczyń': 141,\n",
       " 'adidas': 142,\n",
       " 'córki': 143,\n",
       " 'mistrz': 144,\n",
       " 'zimny': 145,\n",
       " 'ołtarza': 146,\n",
       " 'fajnym': 147,\n",
       " 'znajdujące': 148,\n",
       " 'Jest': 149,\n",
       " 'lider': 150,\n",
       " 'wyjście': 151,\n",
       " 'wyjścia': 152,\n",
       " 'słodkich': 153,\n",
       " 'Przechodzień': 154,\n",
       " 'sprawiają': 155,\n",
       " 'przecież': 156,\n",
       " 'Rocznie': 157,\n",
       " 'odbierają': 158,\n",
       " 'Kolejną': 159,\n",
       " 'Kolejna': 160,\n",
       " 'Kolejny': 161,\n",
       " 'oszpecić': 162,\n",
       " 'wandali': 163,\n",
       " 'ginie': 164,\n",
       " 'podtrzyma': 165,\n",
       " 'uznanie': 166,\n",
       " 'odświeżający': 167,\n",
       " 'odświeżająca': 168,\n",
       " 'podrabianych': 169,\n",
       " 'mdłości': 170,\n",
       " 'W': 171,\n",
       " 'dziwne': 172,\n",
       " 'dziwny': 173,\n",
       " 'sejsmicznej': 174,\n",
       " 'materiału': 175,\n",
       " 'Zachowała': 176,\n",
       " 'odpowiadać': 177,\n",
       " 'odpowiadał': 178,\n",
       " 'nakrętki': 179,\n",
       " 'cielisty': 180,\n",
       " 'Davidoff': 181,\n",
       " 'tanich': 182,\n",
       " 'możliwość': 183,\n",
       " 'markowych': 184,\n",
       " 'zmniejszając': 185,\n",
       " 'Magdę': 186,\n",
       " 'trwa': 187,\n",
       " 'wąskim': 188,\n",
       " 'ludzkości': 189,\n",
       " 'Plus': 190,\n",
       " 'słodkiego': 191,\n",
       " 'zastrzeżeń': 192,\n",
       " 'deski': 193,\n",
       " 'wiruje': 194,\n",
       " 'zbrodniarz': 195,\n",
       " 'złota': 196,\n",
       " 'równocześnie': 197,\n",
       " 'niezapomniane': 198,\n",
       " 'środowisk': 199,\n",
       " 'szanować': 200,\n",
       " 'męskość': 201,\n",
       " 'kwiatowego': 202,\n",
       " 'kadzidła': 203,\n",
       " 'zeznania': 204,\n",
       " 'rocznicy': 205,\n",
       " 'gwarem': 206,\n",
       " 'jedwabisty': 207,\n",
       " 'lubili': 208,\n",
       " 'jakimi': 209,\n",
       " 'urządzenie': 210,\n",
       " 'elektryzującym': 211,\n",
       " 'najbliższej': 212,\n",
       " 'dalej': 213,\n",
       " 'kwiatowa': 214,\n",
       " 'kwiatowo': 215,\n",
       " 'kwiatowy': 216,\n",
       " 'kwiatową': 217,\n",
       " 'Materiał': 218,\n",
       " 'napaść': 219,\n",
       " 'radością': 220,\n",
       " 'starannie': 221,\n",
       " 't': 222,\n",
       " 'charakterem': 223,\n",
       " 'charakterek': 224,\n",
       " 'chronionej': 225,\n",
       " 'historycznej': 226,\n",
       " 'spędzenie': 227,\n",
       " 'zobaczysz': 228,\n",
       " 'młodego': 229,\n",
       " 'najwyższej': 230,\n",
       " 'szklanek': 231,\n",
       " 'nietuzinkowy': 232,\n",
       " 'powiatem': 233,\n",
       " 'pomoc': 234,\n",
       " 'usiąść': 235,\n",
       " 'pomysł': 236,\n",
       " 'stringów': 237,\n",
       " 'Mężczyźni': 238,\n",
       " 'oczekuje': 239,\n",
       " 'Rush': 240,\n",
       " 'etapie': 241,\n",
       " 'korzystnie': 242,\n",
       " 'kandydaci': 243,\n",
       " 'oprawionych': 244,\n",
       " 'używa': 245,\n",
       " 'Stefano': 246,\n",
       " 'miniaturkę': 247,\n",
       " 'kolorystyka': 248,\n",
       " 'gwałtowne': 249,\n",
       " 'Czekaj': 250,\n",
       " 'miesięcy': 251,\n",
       " 'słowa': 252,\n",
       " 'chcą': 253,\n",
       " 'Zwiefkę': 254,\n",
       " 'pozwoliła': 255,\n",
       " 'pazur': 256,\n",
       " 'samonośny': 257,\n",
       " 'Kolorki': 258,\n",
       " 'mycia': 259,\n",
       " 'odnoszę': 260,\n",
       " 'Twój': 261,\n",
       " 'przeciętnego': 262,\n",
       " 'typowej': 263,\n",
       " 'użyciu': 264,\n",
       " 'użycie': 265,\n",
       " 'zanim': 266,\n",
       " 'dotkniętego': 267,\n",
       " 'objęciach': 268,\n",
       " 'przedstawicielach': 269,\n",
       " 'obiekt': 270,\n",
       " 'pewniej': 271,\n",
       " 'zmianą': 272,\n",
       " 'wieczorem': 273,\n",
       " 'łączą': 274,\n",
       " 'swoim': 275,\n",
       " 'spasuje': 276,\n",
       " 'prawdziwa': 277,\n",
       " 'prawdziwy': 278,\n",
       " 'fajnych': 279,\n",
       " 'nieziemski': 280,\n",
       " 'nietrzeźwość': 281,\n",
       " 'dróg': 282,\n",
       " 'kształt': 283,\n",
       " 'kolorową': 284,\n",
       " 'wiele': 285,\n",
       " 'wielu': 286,\n",
       " 'Oświadczył': 287,\n",
       " 'nietrwałe': 288,\n",
       " 'sztuczny': 289,\n",
       " 'Komendy': 290,\n",
       " 'młodej': 291,\n",
       " 'poeta': 292,\n",
       " 'malutka': 293,\n",
       " 'mężem': 294,\n",
       " 'zareklamowany': 295,\n",
       " 'sukienkach': 296,\n",
       " 'zainstalować': 297,\n",
       " 'haftem': 298,\n",
       " 'On': 299,\n",
       " 'Od': 300,\n",
       " 'OK': 301,\n",
       " 'utworzenia': 302,\n",
       " 'uzupełnienie': 303,\n",
       " 'zmysłowość': 304,\n",
       " 'nieprzyzwoitość': 305,\n",
       " 'warunków': 306,\n",
       " 'wiadomo': 307,\n",
       " 'wyrafinowany': 308,\n",
       " 'tymi': 309,\n",
       " 'biustowi': 310,\n",
       " 'snu': 311,\n",
       " 'zdarzyło': 312,\n",
       " 'zdarzyła': 313,\n",
       " 'porządnie': 314,\n",
       " '30ml': 315,\n",
       " 'Panowie': 316,\n",
       " 'próbki': 317,\n",
       " 'Sama': 318,\n",
       " 'Elegancki': 319,\n",
       " 'mdli': 320,\n",
       " 'konserwatywnie': 321,\n",
       " 'sposób': 322,\n",
       " 'polską': 323,\n",
       " 'polska': 324,\n",
       " 'taką': 325,\n",
       " 'taka': 326,\n",
       " 'taki': 327,\n",
       " 'pana': 328,\n",
       " 'pani': 329,\n",
       " 'położeniu': 330,\n",
       " 'utrzymania': 331,\n",
       " 'szli': 332,\n",
       " 'Irga': 333,\n",
       " 'wymienionych': 334,\n",
       " 'wyostrza': 335,\n",
       " 'przykład': 336,\n",
       " 'gęba': 337,\n",
       " 'zakładania': 338,\n",
       " 'brytyjscy': 339,\n",
       " 'lepiej': 340,\n",
       " 'wczesny': 341,\n",
       " 'owocnych': 342,\n",
       " 'pereł': 343,\n",
       " 'wyciszający': 344,\n",
       " 'bałaganu': 345,\n",
       " 'skórzanych': 346,\n",
       " 'chętny': 347,\n",
       " 'pozytywnie': 348,\n",
       " 'zastaw': 349,\n",
       " 'tygodniem': 350,\n",
       " 'zamykaniem': 351,\n",
       " 'podkreśla': 352,\n",
       " 'zdecydowanych': 353,\n",
       " 'podczas': 354,\n",
       " 'ok': 355,\n",
       " 'of': 356,\n",
       " 'od': 357,\n",
       " 'wspaniałej': 358,\n",
       " 'poprzez': 359,\n",
       " 'szczególne': 360,\n",
       " 'piersi': 361,\n",
       " 'Później': 362,\n",
       " 'wojnie': 363,\n",
       " 'posiadam': 364,\n",
       " 'import': 365,\n",
       " 'tego': 366,\n",
       " 'zamówiony': 367,\n",
       " 'pasowała': 368,\n",
       " 'zgodnym': 369,\n",
       " 'prezentuje': 370,\n",
       " 'przyjęła': 371,\n",
       " 'Christian': 372,\n",
       " 'zmysłowego': 373,\n",
       " 'stołecznych': 374,\n",
       " 'twarde': 375,\n",
       " 'muzyce': 376,\n",
       " 'kolacji': 377,\n",
       " 'chociaż': 378,\n",
       " 'szkoda': 379,\n",
       " 'antyutleniacze': 380,\n",
       " 'Ratko': 381,\n",
       " 'bladymi': 382,\n",
       " 'testy': 383,\n",
       " 'Coco': 384,\n",
       " 'szans': 385,\n",
       " 'miseczek': 386,\n",
       " 'kwiatów': 387,\n",
       " 'zapachem': 388,\n",
       " 'energicznej': 389,\n",
       " 'ofercie': 390,\n",
       " 'Niedzieli': 391,\n",
       " 'dizajnie': 392,\n",
       " 'optycznie': 393,\n",
       " 'nagrodach': 394,\n",
       " 'prysznica': 395,\n",
       " 'atrakcyjność': 396,\n",
       " 'Czwartku': 397,\n",
       " 'Generalnie': 398,\n",
       " 'najpiękniejszych': 399,\n",
       " 'duszę': 400,\n",
       " 'duszy': 401,\n",
       " 'zamieniony': 402,\n",
       " 'przereklamowany': 403,\n",
       " 'Przy': 404,\n",
       " 'czarna': 405,\n",
       " 'czarny': 406,\n",
       " 'drogeryjnych': 407,\n",
       " 'czarną': 408,\n",
       " 'Perfumy': 409,\n",
       " 'podnosi': 410,\n",
       " 'inaczej': 411,\n",
       " 'rynku': 412,\n",
       " 'Powązkach': 413,\n",
       " 'tłumu': 414,\n",
       " 'białek': 415,\n",
       " 'Jędrzejowskiego': 416,\n",
       " 'zawsze': 417,\n",
       " 'owocowe': 418,\n",
       " 'lekkie': 419,\n",
       " 'lekkim': 420,\n",
       " 'Frontery': 421,\n",
       " 'takiego': 422,\n",
       " 'porwana': 423,\n",
       " 'Odrobina': 424,\n",
       " 'kontrowersyjnych': 425,\n",
       " 'rozwijały': 426,\n",
       " 'pozorom': 427,\n",
       " 'pomidora': 428,\n",
       " 'Fascynująca': 429,\n",
       " 'uwiera': 430,\n",
       " 'gładkich': 431,\n",
       " 'POLECAM': 432,\n",
       " 'żywić': 433,\n",
       " 'użytkowniczka': 434,\n",
       " 'Idealnie': 435,\n",
       " 'protestów': 436,\n",
       " 'rokowała': 437,\n",
       " 'wyższe': 438,\n",
       " 'łąką': 439,\n",
       " 'alternatywa': 440,\n",
       " 'Całkiem': 441,\n",
       " 'porażką': 442,\n",
       " 'Dzień': 443,\n",
       " 'klientem': 444,\n",
       " 'zabrał': 445,\n",
       " 'Hoop': 446,\n",
       " 'chłopakowi': 447,\n",
       " 'przepiękny': 448,\n",
       " 'światowym': 449,\n",
       " 'eksmitowanych': 450,\n",
       " 'Cenowo': 451,\n",
       " 'zupy': 452,\n",
       " 'Dobrze': 453,\n",
       " 'horrendalna': 454,\n",
       " 'konkurencji': 455,\n",
       " 'głosowania': 456,\n",
       " 'praktyczny': 457,\n",
       " 'praktyczne': 458,\n",
       " 'Większość': 459,\n",
       " 'Napewno': 460,\n",
       " 'rozumieć': 461,\n",
       " 'zdolny': 462,\n",
       " 'wpływem': 463,\n",
       " 'Perfum': 464,\n",
       " 'torebce': 465,\n",
       " 'młodzieżą': 466,\n",
       " 'żądania': 467,\n",
       " 'kontakt': 468,\n",
       " 'głębokich': 469,\n",
       " 'żałuję': 470,\n",
       " 'wypadek': 471,\n",
       " 'Zajmuje': 472,\n",
       " 'pochód': 473,\n",
       " 'zupełnie': 474,\n",
       " 'ładnym': 475,\n",
       " 'drzewny': 476,\n",
       " 'drzewno': 477,\n",
       " 'drzewną': 478,\n",
       " 'znana': 479,\n",
       " 'znane': 480,\n",
       " 'znany': 481,\n",
       " 'nieco': 482,\n",
       " 'porzeczkę': 483,\n",
       " 'genialny': 484,\n",
       " 'rywalizowało': 485,\n",
       " 'ukochanym': 486,\n",
       " 'najbliższy': 487,\n",
       " 'płaskich': 488,\n",
       " 'dojrzałego': 489,\n",
       " 'nieudanych': 490,\n",
       " 'model': 491,\n",
       " 'nakazom': 492,\n",
       " 'rycinie': 493,\n",
       " 'używam': 494,\n",
       " 'używał': 495,\n",
       " 'używać': 496,\n",
       " 'Focusa': 497,\n",
       " 'uroczystego': 498,\n",
       " 'cudownego': 499,\n",
       " 'końcowym': 500,\n",
       " 'wzrok': 501,\n",
       " 'tata': 502,\n",
       " 'przestrzennej': 503,\n",
       " 'trzyma': 504,\n",
       " 'rodziców': 505,\n",
       " 'pyszniła': 506,\n",
       " 'parkiecie': 507,\n",
       " 'żadna': 508,\n",
       " 'żadną': 509,\n",
       " 'domów': 510,\n",
       " 'Pour': 511,\n",
       " 'klasą': 512,\n",
       " 'klasę': 513,\n",
       " 'włożę': 514,\n",
       " 'klasy': 515,\n",
       " 'urzekł': 516,\n",
       " 'urzeka': 517,\n",
       " 'wyglądu': 518,\n",
       " 'wygląda': 519,\n",
       " 'miłośników': 520,\n",
       " 'Polskim': 521,\n",
       " 'pół': 522,\n",
       " 'nieprzewidywalności': 523,\n",
       " 'gubią': 524,\n",
       " 'czegoś': 525,\n",
       " 'uroku': 526,\n",
       " 'prysznic': 527,\n",
       " 'Rosję': 528,\n",
       " 'ramiona': 529,\n",
       " 'sympatykom': 530,\n",
       " 'jakość': 531,\n",
       " 'wypowiedzieli': 532,\n",
       " 'efektywną': 533,\n",
       " 'całymi': 534,\n",
       " 'utrzymywał': 535,\n",
       " 'pożarniczy': 536,\n",
       " 'dyskotekę': 537,\n",
       " 'Wkoło': 538,\n",
       " 'przeminie': 539,\n",
       " 'Cyganeria': 540,\n",
       " 'także': 541,\n",
       " 'Polki': 542,\n",
       " 'Noszę': 543,\n",
       " 'Uniwersytetu': 544,\n",
       " 'Było': 545,\n",
       " 'specyficzny': 546,\n",
       " 'odczuwasz': 547,\n",
       " 'męża': 548,\n",
       " 'egoistycznych': 549,\n",
       " 'zmienił': 550,\n",
       " 'zmienić': 551,\n",
       " 'zmienia': 552,\n",
       " 'mdława': 553,\n",
       " 'czystym': 554,\n",
       " 'efektu': 555,\n",
       " 'imię': 556,\n",
       " 'Kolor': 557,\n",
       " 'najbliższym': 558,\n",
       " 'duchu': 559,\n",
       " 'bunt': 560,\n",
       " 'republiki': 561,\n",
       " 'spotykany': 562,\n",
       " 'kwiatowym': 563,\n",
       " 'zmienione': 564,\n",
       " 'zostawiając': 565,\n",
       " 'Ma': 566,\n",
       " 'Mi': 567,\n",
       " 'strefie': 568,\n",
       " 'uwodzić': 569,\n",
       " 'dystansem': 570,\n",
       " 'odstępy': 571,\n",
       " 'niczym': 572,\n",
       " 'drobnej': 573,\n",
       " 'nutę': 574,\n",
       " 'nuty': 575,\n",
       " 'nuta': 576,\n",
       " 'mediacjach': 577,\n",
       " 'końca': 578,\n",
       " 'koszulce': 579,\n",
       " 'religijni': 580,\n",
       " 'stronę': 581,\n",
       " 'realizmu': 582,\n",
       " 'zagranicznego': 583,\n",
       " 'wykonania': 584,\n",
       " 'przywiązanej': 585,\n",
       " 'zakup': 586,\n",
       " 'ktorą': 587,\n",
       " 'wrażliwą': 588,\n",
       " 'stanie': 589,\n",
       " 'stanik': 590,\n",
       " 'odstrasza': 591,\n",
       " 'Kompletnie': 592,\n",
       " 'przygody': 593,\n",
       " 'Flakonik': 594,\n",
       " 'odparzeń': 595,\n",
       " 'wieczory': 596,\n",
       " 'zbrodnie': 597,\n",
       " 'koloniami': 598,\n",
       " 'intryguje': 599,\n",
       " 'jeszcze': 600,\n",
       " 'obracająca': 601,\n",
       " 'Lauren': 602,\n",
       " 'kłują': 603,\n",
       " 'tytuł': 604,\n",
       " 'wykazuje': 605,\n",
       " 'rozwaliło': 606,\n",
       " 'rozwaliły': 607,\n",
       " 'czasami': 608,\n",
       " 'tknąć': 609,\n",
       " 'jakie': 610,\n",
       " 'pozostawienie': 611,\n",
       " 'obiedzie': 612,\n",
       " 'wzorów': 613,\n",
       " 'szybka': 614,\n",
       " 'szybki': 615,\n",
       " 'szybko': 616,\n",
       " 'dochodzi': 617,\n",
       " 'uzależnił': 618,\n",
       " 'utrzymują': 619,\n",
       " 'przebojowych': 620,\n",
       " 'utrzymuje': 621,\n",
       " 'kark': 622,\n",
       " 'Bogu': 623,\n",
       " 'spokojem': 624,\n",
       " 'kusząco': 625,\n",
       " 'kuszące': 626,\n",
       " 'kuszący': 627,\n",
       " 'Klein': 628,\n",
       " 'samym': 629,\n",
       " 'ulubionych': 630,\n",
       " 'stoi': 631,\n",
       " 'Wyraźnie': 632,\n",
       " 'wiedzą': 633,\n",
       " 'ciepło': 634,\n",
       " 'Buzka': 635,\n",
       " 'absolutnej': 636,\n",
       " 'trwałości': 637,\n",
       " 'ma': 638,\n",
       " 'mi': 639,\n",
       " 'mu': 640,\n",
       " 'zapada': 641,\n",
       " 'seksapil': 642,\n",
       " 'radośnie': 643,\n",
       " 'pięknie': 644,\n",
       " 'utrzyma': 645,\n",
       " 'interesów': 646,\n",
       " 'jakiej': 647,\n",
       " 'brakuje': 648,\n",
       " 'jakieś': 649,\n",
       " 'śliwkę': 650,\n",
       " 'wyczuwalny': 651,\n",
       " 'podkreślone': 652,\n",
       " 'przeznaczony': 653,\n",
       " 'biustonoszach': 654,\n",
       " 'liberalny': 655,\n",
       " 'doniesienia': 656,\n",
       " 'doskonale': 657,\n",
       " 'depresji': 658,\n",
       " 'Niby': 659,\n",
       " 'mnóstwem': 660,\n",
       " 'przechodzi': 661,\n",
       " 'sytuacji': 662,\n",
       " 'wykrochmaloną': 663,\n",
       " 'gust': 664,\n",
       " 'tłumie': 665,\n",
       " 'jednorazówki': 666,\n",
       " 'radość': 667,\n",
       " 'wszytsko': 668,\n",
       " 'Słodkiego': 669,\n",
       " 'uśmiechać': 670,\n",
       " 'wiedział': 671,\n",
       " 'zależy': 672,\n",
       " 'owocujący': 673,\n",
       " 'upiekli': 674,\n",
       " 'odpowiedni': 675,\n",
       " 'Odważna': 676,\n",
       " 'zgięcie': 677,\n",
       " 'radosnym': 678,\n",
       " 'kolorystyce': 679,\n",
       " 'drugiego': 680,\n",
       " 'trzecia': 681,\n",
       " 'nutami': 682,\n",
       " 'rodzinnego': 683,\n",
       " 'Przedsiębiorcę': 684,\n",
       " 'Givenchy': 685,\n",
       " 'mankament': 686,\n",
       " 'kategorii': 687,\n",
       " 'Skierowany': 688,\n",
       " 'upolowania': 689,\n",
       " 'niego': 690,\n",
       " 'Posiada': 691,\n",
       " 'Chociaż': 692,\n",
       " 'liczne': 693,\n",
       " 'liczni': 694,\n",
       " 'ludzie': 695,\n",
       " 'przezroczystą': 696,\n",
       " 'uwielbia': 697,\n",
       " 'podziękowania': 698,\n",
       " 'włoski': 699,\n",
       " 'Ładniejszy': 700,\n",
       " 'Pań': 701,\n",
       " 'Pan': 702,\n",
       " 'zajmuje': 703,\n",
       " 'napadem': 704,\n",
       " 'zaskakujący': 705,\n",
       " 'z': 706,\n",
       " 'niska': 707,\n",
       " 'ściśnięcia': 708,\n",
       " 'zaczęły': 709,\n",
       " 'Mühlhausowi': 710,\n",
       " 'spełnił': 711,\n",
       " 'policja': 712,\n",
       " 'policji': 713,\n",
       " 'Marka': 714,\n",
       " 'cudna': 715,\n",
       " 'doświadczają': 716,\n",
       " 'starym': 717,\n",
       " 'groźne': 718,\n",
       " 'kotom': 719,\n",
       " 'wyglądać': 720,\n",
       " 'Dune': 721,\n",
       " 'wystąpienie': 722,\n",
       " 'fasonów': 723,\n",
       " 'kłodzkiego': 724,\n",
       " 'skromne': 725,\n",
       " 'skromny': 726,\n",
       " 'nietuzinkowym': 727,\n",
       " 'Muszę': 728,\n",
       " 'zakupów': 729,\n",
       " 'rodzinę': 730,\n",
       " 'rodziną': 731,\n",
       " 'rodzina': 732,\n",
       " 'Wizytek': 733,\n",
       " 'mililitrów': 734,\n",
       " 'moja': 735,\n",
       " 'romantyczki': 736,\n",
       " 'szczególnego': 737,\n",
       " 'oddała': 738,\n",
       " 'ekspertów': 739,\n",
       " 'istniała': 740,\n",
       " 'okazyjnej': 741,\n",
       " 'rannych': 742,\n",
       " 'sąsiedzi': 743,\n",
       " 'najdroższe': 744,\n",
       " 'Kujawy': 745,\n",
       " 'perfumach': 746,\n",
       " 'takim': 747,\n",
       " 'stosunku': 748,\n",
       " 'piłkarskiej': 749,\n",
       " 'powojennej': 750,\n",
       " 'bycia': 751,\n",
       " 'zapewnia': 752,\n",
       " 'pikanterii': 753,\n",
       " 'wpisała': 754,\n",
       " 'wydajne': 755,\n",
       " 'wspomnienia': 756,\n",
       " 'toczą': 757,\n",
       " 'ścisnąć': 758,\n",
       " 'Szczególnie': 759,\n",
       " 'dziękuję': 760,\n",
       " 'kogoś': 761,\n",
       " 'koleżankę': 762,\n",
       " 'duże': 763,\n",
       " 'koleżanki': 764,\n",
       " 'zwiększając': 765,\n",
       " 'kreacje': 766,\n",
       " 'poranku': 767,\n",
       " 'odszkodowania': 768,\n",
       " 'poszukuję': 769,\n",
       " 'staniczek': 770,\n",
       " 'godnym': 771,\n",
       " 'tradycyjnym': 772,\n",
       " 'całemu': 773,\n",
       " 'mocna': 774,\n",
       " 'znanej': 775,\n",
       " 'zasłania': 776,\n",
       " 'odnieść': 777,\n",
       " 'Ksiądz': 778,\n",
       " 'powietrzu': 779,\n",
       " 'powietrza': 780,\n",
       " 'powietrze': 781,\n",
       " 'amarantowa': 782,\n",
       " 'Tymczasem': 783,\n",
       " 'całą': 784,\n",
       " 'leży': 785,\n",
       " 'cały': 786,\n",
       " 'Cmentarza': 787,\n",
       " 'gatunkowo': 788,\n",
       " 'spytało': 789,\n",
       " 'buja': 790,\n",
       " 'Śliczny': 791,\n",
       " 'Śliczna': 792,\n",
       " 'Biernacki': 793,\n",
       " 'energicznych': 794,\n",
       " 'własną': 795,\n",
       " 'złożył': 796,\n",
       " 'żony': 797,\n",
       " 'żona': 798,\n",
       " 'partii': 799,\n",
       " 'ani': 800,\n",
       " 'wibrującą': 801,\n",
       " 'reklama': 802,\n",
       " 'macochy': 803,\n",
       " 'flaszka': 804,\n",
       " 'beznadziejny': 805,\n",
       " 'niszcząc': 806,\n",
       " 'modeluje': 807,\n",
       " 'modelują': 808,\n",
       " 'lekką': 809,\n",
       " 'własnych': 810,\n",
       " 'niespodzianek': 811,\n",
       " 'Zachowują': 812,\n",
       " 'lekko': 813,\n",
       " 'lekki': 814,\n",
       " 'Zakupiła': 815,\n",
       " 'tracąc': 816,\n",
       " 'Prokuratury': 817,\n",
       " 'trucizna': 818,\n",
       " 'pełne': 819,\n",
       " 'odpowiada': 820,\n",
       " 'ułożenia': 821,\n",
       " 'osobiście': 822,\n",
       " 'pasku': 823,\n",
       " 'sporo': 824,\n",
       " 'spora': 825,\n",
       " 'spory': 826,\n",
       " 'sporą': 827,\n",
       " 'Beatki': 828,\n",
       " 'nadgarstkowe': 829,\n",
       " 'Oczywiście': 830,\n",
       " 'podążyła': 831,\n",
       " 'Armani': 832,\n",
       " 'sytuacjami': 833,\n",
       " 'niewielki': 834,\n",
       " 'bezpieczeństwa': 835,\n",
       " 'przy': 836,\n",
       " 'codzień': 837,\n",
       " 'codzien': 838,\n",
       " 'ocenili': 839,\n",
       " 'części': 840,\n",
       " 'stołka': 841,\n",
       " 'wspaniałego': 842,\n",
       " 'esencjonalny': 843,\n",
       " 'zginie': 844,\n",
       " 'rozkrzewiona': 845,\n",
       " 'dekolcie': 846,\n",
       " 'zakrętka': 847,\n",
       " 'zdecydowanie': 848,\n",
       " 'For': 849,\n",
       " 'waniliowy': 850,\n",
       " 'śmy': 851,\n",
       " 'nigdy': 852,\n",
       " 'własnego': 853,\n",
       " 'domu': 854,\n",
       " 'uwielbiają': 855,\n",
       " 'pieniądze': 856,\n",
       " 'postkomuch': 857,\n",
       " 'żydowskich': 858,\n",
       " 'wypróbowała': 859,\n",
       " 'Cieszą': 860,\n",
       " 'Stan': 861,\n",
       " 'Kolorystyka': 862,\n",
       " 'nowymi': 863,\n",
       " 'wiązanie': 864,\n",
       " 'wizerunkiem': 865,\n",
       " 'użytkowania': 866,\n",
       " 'Komor': 867,\n",
       " 'berecie': 868,\n",
       " 'zapachową': 869,\n",
       " 'zapachowe': 870,\n",
       " 'zapachowa': 871,\n",
       " 'zaprojektowane': 872,\n",
       " 'sylwetkę': 873,\n",
       " 'sylwetki': 874,\n",
       " 'zawiedzie': 875,\n",
       " 'wicestarosta': 876,\n",
       " 'kształcie': 877,\n",
       " 'urzędzie': 878,\n",
       " 'letnie': 879,\n",
       " 'letnia': 880,\n",
       " 'znaleźć': 881,\n",
       " 'rząd': 882,\n",
       " 'jesienne': 883,\n",
       " 'jesienną': 884,\n",
       " 'róży': 885,\n",
       " 'różu': 886,\n",
       " 'Jesteśmy': 887,\n",
       " 'bergamotki': 888,\n",
       " 'nocną': 889,\n",
       " 'piersiami': 890,\n",
       " 'ludzkiej': 891,\n",
       " 'rozciąga': 892,\n",
       " 'nieciekawy': 893,\n",
       " 'końcu': 894,\n",
       " 'arbitra': 895,\n",
       " 'optymizmu': 896,\n",
       " 'Całokształt': 897,\n",
       " 'udało': 898,\n",
       " 'Potem': 899,\n",
       " 'wakacje': 900,\n",
       " 'chorobami': 901,\n",
       " 'Lwowskich': 902,\n",
       " 'ciężki': 903,\n",
       " 'ustach': 904,\n",
       " 'przed': 905,\n",
       " 'mokrym': 906,\n",
       " 'dziedzinach': 907,\n",
       " 'diametralnie': 908,\n",
       " 'witali': 909,\n",
       " 'przecudne': 910,\n",
       " 'przecudny': 911,\n",
       " 'nadawania': 912,\n",
       " 'Znalezisko': 913,\n",
       " 'długi': 914,\n",
       " 'długo': 915,\n",
       " 'krój': 916,\n",
       " 'zostawiam': 917,\n",
       " 'kokarda': 918,\n",
       " 'Spodziewał': 919,\n",
       " 'Majteczki': 920,\n",
       " 'słodszych': 921,\n",
       " 'jesiennym': 922,\n",
       " 'wyniesione': 923,\n",
       " 'wyszczuplającego': 924,\n",
       " 'używając': 925,\n",
       " 'euforia': 926,\n",
       " 'euforię': 927,\n",
       " 'silniejsze': 928,\n",
       " 'cytrusami': 929,\n",
       " 'wanien': 930,\n",
       " 'silnej': 931,\n",
       " 'Mimo': 932,\n",
       " 'większe': 933,\n",
       " 'większą': 934,\n",
       " 'perfum': 935,\n",
       " 'pralce': 936,\n",
       " 'Założony': 937,\n",
       " 'torebki': 938,\n",
       " 'dotyk': 939,\n",
       " 'czymś': 940,\n",
       " 'dużym': 941,\n",
       " 'związkach': 942,\n",
       " 'prezentacji': 943,\n",
       " 'brutalni': 944,\n",
       " 'ręcznie': 945,\n",
       " 'pełnowymiarowego': 946,\n",
       " 'Obecnie': 947,\n",
       " 'wprawia': 948,\n",
       " 'Kupuję': 949,\n",
       " 'Pi': 950,\n",
       " 'Po': 951,\n",
       " 'ludobójstwo': 952,\n",
       " 'obserwatorzy': 953,\n",
       " 'Stanik': 954,\n",
       " 'rozwija': 955,\n",
       " 'chciała': 956,\n",
       " 'izolacji': 957,\n",
       " 'rywalem': 958,\n",
       " 'pojechać': 959,\n",
       " 'ostatecznego': 960,\n",
       " 'niezbyt': 961,\n",
       " 'zna': 962,\n",
       " 'Przepiękne': 963,\n",
       " 'Przepiękny': 964,\n",
       " 'niezwykle': 965,\n",
       " 'ostrą': 966,\n",
       " 'Kusi': 967,\n",
       " 'ostre': 968,\n",
       " 'ostry': 969,\n",
       " 'ks': 970,\n",
       " 'ku': 971,\n",
       " 'Kiedy': 972,\n",
       " 'przekombinowany': 973,\n",
       " 'dekoracyjny': 974,\n",
       " 'podkreślają': 975,\n",
       " 'D': 976,\n",
       " 'proszą': 977,\n",
       " 'gdzie': 978,\n",
       " 'podobał': 979,\n",
       " 'podobać': 980,\n",
       " 'zakomunikowali': 981,\n",
       " 'niespecjalnie': 982,\n",
       " 'fanów': 983,\n",
       " 'światowa': 984,\n",
       " 'widnieją': 985,\n",
       " 'postanowiła': 986,\n",
       " 'owa': 987,\n",
       " 'kolorów': 988,\n",
       " 'otrzymała': 989,\n",
       " 'nagminnie': 990,\n",
       " 'zalety': 991,\n",
       " 'Młodych': 992,\n",
       " 'wyglądem': 993,\n",
       " 'majtek': 994,\n",
       " 'przesłodzony': 995,\n",
       " 'Wracam': 996,\n",
       " 'aromaty': 997,\n",
       " 'czesto': 998,\n",
       " 'wyprofilowana': 999,\n",
       " 'szczyt': 1000,\n",
       " 'tytoniu': 1001,\n",
       " ...}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words2ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 300)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "?Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence as seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_transform_data_to_phrases(labels, parents, tokens, words2ids):\n",
    "\n",
    "    \"\"\"\n",
    "    Dokumentacja\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    transform_label = {'-1':0, '0':1, '1':2}\n",
    "    \n",
    "    l = open(labels, \"r\")\n",
    "    labels = [[transform_label[y] for y in x.split()] for x in l.readlines()] \n",
    "    l.close()\n",
    "\n",
    "    p = open(parents,\"r\")\n",
    "    parents = [[int(y) for y in x.split()] for x in p.readlines()]\n",
    "    p.close()\n",
    "\n",
    "    t = open(tokens,\"r\")\n",
    "    tokens = [x.split() for x in t.readlines()]\n",
    "    t.close()\n",
    "    \n",
    "    k = 0\n",
    "    result = []\n",
    "    \n",
    "    for labels_i,parents_i,tokens_i in zip(labels,parents,tokens):\n",
    "        \n",
    "        k = k + 1\n",
    "         \n",
    "        s = []\n",
    "        for i in range(len(tokens_i)):\n",
    "            s.append([i,int(parents_i[i]),labels_i[i],tokens_i[i]])\n",
    "\n",
    "\n",
    "        if len(s) == 1: #przypadek gdy fraza sklada sie z jednego tokena\n",
    "\n",
    "            result.append((\\\n",
    "                                  tokens[0],\n",
    "                                  np.array([words2ids.get(tokens[0], 1)]),\\\n",
    "                                  np.array(labels_i[0]) \\\n",
    "                              ))    \n",
    "                           \n",
    "        else: \n",
    "            \n",
    "            for i in range(len(s)): \n",
    "                children = []\n",
    "                for j in range(len(s)):\n",
    "                    if s[j][1] == i+1:\n",
    "                        children.append(s[j][0])\n",
    "                s[i].append(children)\n",
    "\n",
    "                \n",
    "            words = [x[0] for x in s]\n",
    "            children = [x[4] for x in s]\n",
    "            tokens = [x[3] for x in s]\n",
    "            labels_in_batch = [x[2] for x in s]\n",
    "        \n",
    "            phrases = [[k] for k in range(len(children))]\n",
    "            for i in range(len(children)):\n",
    "                for e in phrases[i]:\n",
    "                    phrases[i].extend(children[e])\n",
    "           \n",
    "            phrases = [ np.sort(x) for x in phrases]\n",
    "          \n",
    "            phrases = list(zip([np.array(tokens_i)[x] for x in phrases],\n",
    "                               [np.array([words2ids.get(t,1) for t in tokens_i])[x] for x in phrases],\n",
    "                               labels_i))\n",
    "\n",
    "            result.extend(phrases)\n",
    "           \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_and_transform_data_to_phrases(\"Dane/data_poleval/training-treebank/rev_labels.txt\", \"Dane/data_poleval/training-treebank/rev_parents.txt\",\"Dane/data_poleval/training-treebank/rev_sentence.txt\",words2ids)\n",
    "test_data = load_and_transform_data_to_phrases(\"Dane/data_poleval/poleval_test/gold_labels\", \"Dane/data_poleval/poleval_test/polevaltest_parents.txt\",\"Dane/data_poleval/poleval_test/polevaltest_sentence.txt\",words2ids) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Słodkawy'], dtype='<U8'), array([2731]), 1)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = np.array([x[0] for x in train_data])\n",
    "text_test = np.array([x[0] for x in test_data])\n",
    "\n",
    "maxlen = max(max([len(text) for text in text_train]), max([len(text) for text in text_test]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([2731]), array([2731, 1746, 1465,  515,    1]),\n",
       "       array([1465]), array([1465,  515]), array([1])], dtype=object)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_train = np.array([x[0] for x in test_data])\n",
    "X_test = np.array([x[1] for x in test_data])\n",
    "X_train = np.array([x[1] for x in train_data])\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9510,)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, 2, 1])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_y_test = np.array([x[2] for x in test_data])\n",
    "pre_y_train = np.array([x[2] for x in train_data])\n",
    "pre_y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_test = to_categorical(pre_y_test)\n",
    "y_train = to_categorical(pre_y_train)\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([2731]), 1], dtype=object)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2421"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features = int(1 * len(set(np.concatenate([list(x[0]) for x in train_data]))))\n",
    "max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9510, 40)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_pad = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "X_train_pad = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2731])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wykorzystanie gotowych (*pretrained*) embeddingów: \n",
    "\n",
    "Embedding(..., weights = [embeddings], trainable = True/False)\n",
    "\n",
    "Parametr `trainable` - określa czy chcemy douczać nasze embeddingi czy nie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 5000\n",
    "embedding_dims = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_25 (Embedding)     (None, 40, 300)           1500000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 10)                12440     \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 1,512,473\n",
      "Trainable params: 12,473\n",
      "Non-trainable params: 1,500,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add( Embedding( max_features, embedding_dims, input_length=maxlen, weights=[embeddings], trainable=False ) )\n",
    "model.add( LSTM(10) )\n",
    "model.add( Dense(3 ,activation='softmax') )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9510, 3)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9510, 40)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7608 samples, validate on 1902 samples\n",
      "Epoch 1/100\n",
      "7608/7608 [==============================] - 13s 2ms/step - loss: 0.1253 - acc: 0.9606 - val_loss: 0.3864 - val_acc: 0.8828\n",
      "Epoch 2/100\n",
      "7608/7608 [==============================] - 12s 2ms/step - loss: 0.1140 - acc: 0.9648 - val_loss: 0.4162 - val_acc: 0.8791\n",
      "Epoch 3/100\n",
      "7608/7608 [==============================] - 12s 2ms/step - loss: 0.1047 - acc: 0.9695 - val_loss: 0.4122 - val_acc: 0.8801\n",
      "Epoch 4/100\n",
      "7608/7608 [==============================] - 13s 2ms/step - loss: 0.0960 - acc: 0.9724 - val_loss: 0.4260 - val_acc: 0.8770\n",
      "5047/5047 [==============================] - 1s 268us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7508421956441745, 0.7925500296379567]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(patience=3, monitor=\"val_acc\")\n",
    "model.fit(X_train_pad, y_train, epochs=100, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "model.evaluate(X_test_pad, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_26 (Embedding)     (None, 40, 300)           1500000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 38, 50)            45050     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 10)                2440      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 3)                 33        \n",
      "=================================================================\n",
      "Total params: 1,547,523\n",
      "Trainable params: 47,523\n",
      "Non-trainable params: 1,500,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add( Embedding( max_features, embedding_dims, input_length=maxlen, weights=[embeddings], trainable=False ) )\n",
    "model.add( Conv1D(50,3) )\n",
    "model.add( LSTM(10) )\n",
    "model.add( Dense(3 ,activation='softmax') )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7608 samples, validate on 1902 samples\n",
      "Epoch 1/100\n",
      "7608/7608 [==============================] - 14s 2ms/step - loss: 0.4566 - acc: 0.8408 - val_loss: 0.3877 - val_acc: 0.8644\n",
      "Epoch 2/100\n",
      "7608/7608 [==============================] - 12s 2ms/step - loss: 0.2505 - acc: 0.9119 - val_loss: 0.3788 - val_acc: 0.8717\n",
      "Epoch 3/100\n",
      "7608/7608 [==============================] - 13s 2ms/step - loss: 0.1915 - acc: 0.9366 - val_loss: 0.3564 - val_acc: 0.8754\n",
      "Epoch 4/100\n",
      "7608/7608 [==============================] - 14s 2ms/step - loss: 0.1515 - acc: 0.9507 - val_loss: 0.3606 - val_acc: 0.8801\n",
      "Epoch 5/100\n",
      "7608/7608 [==============================] - 14s 2ms/step - loss: 0.1237 - acc: 0.9627 - val_loss: 0.3728 - val_acc: 0.8754\n",
      "Epoch 6/100\n",
      "7608/7608 [==============================] - 14s 2ms/step - loss: 0.1031 - acc: 0.9682 - val_loss: 0.3817 - val_acc: 0.8864\n",
      "Epoch 7/100\n",
      "7608/7608 [==============================] - 15s 2ms/step - loss: 0.0893 - acc: 0.9728 - val_loss: 0.4024 - val_acc: 0.8828\n",
      "Epoch 8/100\n",
      "7608/7608 [==============================] - 15s 2ms/step - loss: 0.0790 - acc: 0.9769 - val_loss: 0.4211 - val_acc: 0.8859\n",
      "Epoch 9/100\n",
      "7608/7608 [==============================] - 14s 2ms/step - loss: 0.0691 - acc: 0.9813 - val_loss: 0.4159 - val_acc: 0.8901\n",
      "Epoch 10/100\n",
      "7608/7608 [==============================] - 14s 2ms/step - loss: 0.0618 - acc: 0.9828 - val_loss: 0.4686 - val_acc: 0.8828\n",
      "Epoch 11/100\n",
      "7608/7608 [==============================] - 14s 2ms/step - loss: 0.0551 - acc: 0.9850 - val_loss: 0.4715 - val_acc: 0.8807\n",
      "Epoch 12/100\n",
      "7608/7608 [==============================] - 14s 2ms/step - loss: 0.0492 - acc: 0.9869 - val_loss: 0.4792 - val_acc: 0.8833\n",
      "5047/5047 [==============================] - 2s 305us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8483443292433941, 0.7933425796676773]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(patience=3, monitor=\"val_acc\")\n",
    "model.fit(X_train_pad, y_train, epochs=100, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "model.evaluate(X_test_pad, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add( Embedding( max_features, embedding_dims, input_length=maxlen, weights=[embeddings], trainable=False ) )\n",
    "model.add( TimeDistributed(Dense(15)) )\n",
    "model.add( LSTM(10) )\n",
    "model.add( Dense(3 ,activation='softmax') )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Modelowanie szeregów czasowych\n",
    "\n",
    "https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "dataframe = pandas.read_csv('Dane/international-airline-passengers.csv', usecols=[1], engine='python', skipfooter=3)\n",
    "plt.plot(dataframe)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(dataset)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 48\n"
     ]
    }
   ],
   "source": [
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn numpy.array(dataX), numpy.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 3\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.01544401, 0.02702703, 0.05405405]],\n",
       "\n",
       "       [[0.02702703, 0.05405405, 0.04826255]],\n",
       "\n",
       "       [[0.05405405, 0.04826255, 0.03281853]],\n",
       "\n",
       "       [[0.04826255, 0.03281853, 0.05984557]],\n",
       "\n",
       "       [[0.03281853, 0.05984557, 0.08494207]],\n",
       "\n",
       "       [[0.05984557, 0.08494207, 0.08494207]],\n",
       "\n",
       "       [[0.08494207, 0.08494207, 0.06177607]],\n",
       "\n",
       "       [[0.08494207, 0.06177607, 0.02895753]],\n",
       "\n",
       "       [[0.06177607, 0.02895753, 0.        ]],\n",
       "\n",
       "       [[0.02895753, 0.        , 0.02702703]]], dtype=float32)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.4073359 , 0.3803089 , 0.48648646]],\n",
       "\n",
       "       [[0.3803089 , 0.48648646, 0.47104248]],\n",
       "\n",
       "       [[0.48648646, 0.47104248, 0.484556  ]],\n",
       "\n",
       "       [[0.47104248, 0.484556  , 0.6138996 ]],\n",
       "\n",
       "       [[0.484556  , 0.6138996 , 0.6969112 ]],\n",
       "\n",
       "       [[0.6138996 , 0.6969112 , 0.70077217]],\n",
       "\n",
       "       [[0.6969112 , 0.70077217, 0.57915056]],\n",
       "\n",
       "       [[0.70077217, 0.57915056, 0.46911195]],\n",
       "\n",
       "       [[0.57915056, 0.46911195, 0.38803086]],\n",
       "\n",
       "       [[0.46911195, 0.38803086, 0.44787642]]], dtype=float32)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04826255, 0.03281853, 0.05984557, 0.08494207, 0.08494207,\n",
       "       0.06177607, 0.02895753, 0.        , 0.02702703, 0.02123553],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 82 samples, validate on 10 samples\n",
      "Epoch 1/100\n",
      " - 2s - loss: 0.0377 - val_loss: 0.1438\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.0171 - val_loss: 0.0868\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.0118 - val_loss: 0.0619\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.0104 - val_loss: 0.0544\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.0096 - val_loss: 0.0471\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.0088 - val_loss: 0.0404\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.0080 - val_loss: 0.0388\n",
      "Epoch 8/100\n",
      " - 1s - loss: 0.0074 - val_loss: 0.0299\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.0067 - val_loss: 0.0268\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.0061 - val_loss: 0.0241\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.0055 - val_loss: 0.0211\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.0051 - val_loss: 0.0203\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.0047 - val_loss: 0.0180\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.0044 - val_loss: 0.0165\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.0042 - val_loss: 0.0152\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.0040 - val_loss: 0.0133\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.0038 - val_loss: 0.0134\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.0037 - val_loss: 0.0131\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.0036 - val_loss: 0.0137\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.0036 - val_loss: 0.0126\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.0035 - val_loss: 0.0119\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.0036 - val_loss: 0.0115\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.0034 - val_loss: 0.0119\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.0033 - val_loss: 0.0116\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.0032 - val_loss: 0.0119\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.0032 - val_loss: 0.0112\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.0031 - val_loss: 0.0115\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.0031 - val_loss: 0.0110\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.0031 - val_loss: 0.0110\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.0031 - val_loss: 0.0110\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.0030 - val_loss: 0.0107\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.0030 - val_loss: 0.0106\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.0030 - val_loss: 0.0106\n",
      "Epoch 34/100\n",
      " - 1s - loss: 0.0028 - val_loss: 0.0109\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.0030 - val_loss: 0.0102\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.0029 - val_loss: 0.0101\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.0029 - val_loss: 0.0100\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.0029 - val_loss: 0.0100\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.0029 - val_loss: 0.0098\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.0028 - val_loss: 0.0097\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.0029 - val_loss: 0.0095\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.0028 - val_loss: 0.0094\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.0027 - val_loss: 0.0093\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.0027 - val_loss: 0.0092\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.0027 - val_loss: 0.0091\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.0028 - val_loss: 0.0090\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.0026 - val_loss: 0.0088\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.0027 - val_loss: 0.0088\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.0027 - val_loss: 0.0090\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.0025 - val_loss: 0.0085\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.0025 - val_loss: 0.0084\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.0026 - val_loss: 0.0083\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.0024 - val_loss: 0.0081\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.0025 - val_loss: 0.0080\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.0024 - val_loss: 0.0079\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.0024 - val_loss: 0.0078\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.0024 - val_loss: 0.0078\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.0023 - val_loss: 0.0077\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.0023 - val_loss: 0.0077\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.0024 - val_loss: 0.0075\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.0023 - val_loss: 0.0073\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.0023 - val_loss: 0.0072\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.0023 - val_loss: 0.0071\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.0023 - val_loss: 0.0073\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.0022 - val_loss: 0.0069\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.0022 - val_loss: 0.0069\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.0022 - val_loss: 0.0067\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.0022 - val_loss: 0.0071\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.0022 - val_loss: 0.0066\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.0022 - val_loss: 0.0064\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.0021 - val_loss: 0.0064\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.0022 - val_loss: 0.0065\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.0021 - val_loss: 0.0062\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.0021 - val_loss: 0.0062\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.0021 - val_loss: 0.0062\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.0020 - val_loss: 0.0059\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.0021 - val_loss: 0.0059\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.0021 - val_loss: 0.0060\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.0020 - val_loss: 0.0058\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.0020 - val_loss: 0.0057\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.0020 - val_loss: 0.0065\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.0020 - val_loss: 0.0055\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.0021 - val_loss: 0.0055\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.0020 - val_loss: 0.0055\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.0020 - val_loss: 0.0053\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.0019 - val_loss: 0.0052\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.0020 - val_loss: 0.0052\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.0020 - val_loss: 0.0051\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.0019 - val_loss: 0.0053\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.0019 - val_loss: 0.0050\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.0019 - val_loss: 0.0050\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.0019 - val_loss: 0.0052\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.0019 - val_loss: 0.0050\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.0019 - val_loss: 0.0048\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.0020 - val_loss: 0.0048\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.0019 - val_loss: 0.0048\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.0020 - val_loss: 0.0048\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.0018 - val_loss: 0.0046\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6917082c88>"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 23.97 RMSE\n",
      "Test Score: 47.78 RMSE\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
